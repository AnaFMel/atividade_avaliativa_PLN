{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Atividade Avaliativa - Processamento de Linguagem Natural**\n",
        "### **Equipe:** Ana Clara, Bruno, Igor e Pedro Cruz\n"
      ],
      "metadata": {
        "id": "tb_xoKytbTNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descrição e motivação do problema:**\n",
        "\n",
        "A disseminação de **fake news** tem acontecido com cada vez mais frequência e possui impacto significativo na sociedade, contribuindo para a desinformação, polarização social e decisões equivocadas. A motivação para desenvolver um algoritmo de identificação de notícias falsas está em promover uma sociedade mais informada, fornecendo as pessoas uma ferramenta automatizada para avaliar a veracidade de informações rapidamente. Isso pode ajudar a mitigar os efeitos negativos da desinformação, capacitando as pessoas a tomarem decisões mais conscientes e baseadas em fatos confiáveis."
      ],
      "metadata": {
        "id": "N35uvHTFb_vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descrição da base de dados:**\n",
        "\n",
        "A base de dados que decidimos utilizar é a [Fake.Br Corpus](https://github.com/roneysco/Fake.br-Corpus). Ela fornece pares de notícias verdadeiras e falsas em português.\n",
        "\n",
        "Mais especificamente, iremos utilizar os dados contidos na pasta \"size_normalized_texts\", que contém versões truncadas dos textos, onde em cada par verdadeiro-falso o texto mais longo é truncado (em número de palavras) para o tamanho do texto mais curto. Esta versão do corpus pode ser útil para evitar inclinações incorretas em experimentos de aprendizado de máquina."
      ],
      "metadata": {
        "id": "FVOFfOvPdNuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivo de negócio ou científico associado ao problema:**\n",
        "\n",
        "\n",
        "O objetivo científico deste projeto é desenvolver e validar um algoritmo capaz de identificar fake news baseado no que ele sabe. Já o objetivo de negócio é fornecer uma solução automatizada, que permita organizações, jornalistas e usuários individuais verificarem rapidamente a veracidade de informações, reduzindo os impactos da desinformação em escala e promovendo a confiança em fontes confiáveis."
      ],
      "metadata": {
        "id": "XWYJXJcxdWII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-processamento & Extração de Características:**"
      ],
      "metadata": {
        "id": "o5gSBds0dlB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fazendo todas as importações necessárias para o notebook"
      ],
      "metadata": {
        "id": "bjZWjCcigBi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "wdtIM0Wab9tU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipando os arquivos baixados"
      ],
      "metadata": {
        "id": "o2QmvS_-pSr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_zip = 'Fake.br-Corpus-master.zip'\n",
        "\n",
        "with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('arquivos')"
      ],
      "metadata": {
        "id": "l5Lnbxo6oPjj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtendos os dados da pasta size_normalized_texts, como comentado anteriormente, e colocando tudo em um dataframe só, inserindo o label 0 ou 1 para distinguirmos entre verdadeiro ou falso."
      ],
      "metadata": {
        "id": "8sac4bHEqEwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_dir = 'arquivos/Fake.br-Corpus-master/size_normalized_texts/fake'\n",
        "true_dir = 'arquivos/Fake.br-Corpus-master/size_normalized_texts/true'\n",
        "\n",
        "def load_texts(directory, label):\n",
        "    data = []\n",
        "    for file in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, file)\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().strip()\n",
        "                if text:\n",
        "                    data.append({'text': text, 'label': label})\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao ler o arquivo {file_path}: {e}\")\n",
        "    return data\n",
        "\n",
        "fake_data = load_texts(fake_dir, 0)  # 0 para \"falso\"\n",
        "true_data = load_texts(true_dir, 1)  # 1 para \"verdadeiro\"\n",
        "\n",
        "df = pd.DataFrame(fake_data + true_data)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2-jiWnzpW2M",
        "outputId": "66e0898c-f25f-47f5-c2d8-f7d7d4818aa8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  Dilma escorrega em entrevista e confessa que p...      0\n",
            "1  Wikileaks diz que Temer era espião dos EUA. . ...      0\n",
            "2  Jornalista divulga vídeo no whatsapp mostra a ...      0\n",
            "3  Em edição do JN, Willian Bonner faz pergunta c...      0\n",
            "4  ISOLADO: Lula vive dias de depressão e já admi...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisando tamanho"
      ],
      "metadata": {
        "id": "HuRnOUTYsbU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8u1tlSzseSv",
        "outputId": "268f2797-4634-4220-f268-aa160fca0a75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7200, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando se temos algum nulo"
      ],
      "metadata": {
        "id": "ju7rVo1Asp7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vozOIenjsfLv",
        "outputId": "1ea4fdaf-2873-4d0b-aeb0-edbaa31bd7ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7200 entries, 0 to 7199\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    7200 non-null   object\n",
            " 1   label   7200 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 112.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixando a library dos transformers"
      ],
      "metadata": {
        "id": "u9X_6vTugfMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cAB91WmdUiC",
        "outputId": "b72d1aa2-369f-477c-ed0a-bb0dca78ef1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializando o tokenizador para transformar textos em tokens numéricos e carregando o modelo pré-treinado BERT para extração de características."
      ],
      "metadata": {
        "id": "GqWyLLCKzdDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "MAX_LEN = 128\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9lYS1XHqfYU",
        "outputId": "14aecffc-b7db-43b3-8964-b9314077f993"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando função para gerar embeddings dos textos processados. Primeiro, os textos são tokenizados, truncados ou preenchidos para atingir o comprimento máximo (max_len), depois são convertidos em tensores PyTorch. Esses tensores, incluindo inpuit_ids e attention_masks, são organizados em um DataLoader para processamento em lotes com o modelo BERT. No loop, o modelo gera a saída, da qual é extraída a embedding correspondente ao token [CLS]. Esses embeddings são acumulados, convertidos em arrays NumPy, concatenados e retornados como um único array que representa todas as entradas."
      ],
      "metadata": {
        "id": "8WJx7sizzzw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(texts, tokenizer, model, max_len, batch_size):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    dataset = TensorDataset(encoded['input_ids'], encoded['attention_mask'])\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask in dataloader:\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "            embeddings.append(cls_embedding.cpu().numpy())\n",
        "    return np.vstack(embeddings)"
      ],
      "metadata": {
        "id": "dOB8XWYBrqvi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtendo os embeddings:"
      ],
      "metadata": {
        "id": "YG5l7MoT0uS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_file = \"embeddings.npy\"\n",
        "\n",
        "if os.path.exists(embeddings_file):\n",
        "    embeddings = np.load(embeddings_file)\n",
        "else:\n",
        "    texts = df['text'].tolist()\n",
        "    batch_size = 32\n",
        "    embeddings = generate_embeddings(texts, tokenizer, bert_model, MAX_LEN, batch_size)\n",
        "    np.save(\"embeddings.npy\", embeddings)"
      ],
      "metadata": {
        "id": "osXU_vcnt0-h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo de Machine Learning:**"
      ],
      "metadata": {
        "id": "1dv8DrjOduyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['label'].tolist()\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "_XbP4Evxt6x8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Protocolo de experimentos e validação**"
      ],
      "metadata": {
        "id": "8xiN3HG2dy3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['falso', 'verdadeiro']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMei79pit90b",
        "outputId": "1dec5c5b-3021-48ee-aa5e-bc5635fc5bfb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       falso       0.96      0.96      0.96       718\n",
            "  verdadeiro       0.96      0.96      0.96       722\n",
            "\n",
            "    accuracy                           0.96      1440\n",
            "   macro avg       0.96      0.96      0.96      1440\n",
            "weighted avg       0.96      0.96      0.96      1440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo, criamos uma função para facilitar o processo de testes do modelo com dados novos. Definimos que caso seja fornecido uma frase de entrada completamente fora do escopo do que o modelo conhece ou caso ele não tenha muita certeza, ele deve retornar que não sabe se a notícia é verdade ou mentira, para que não contribua acidentamente para propagação de desinformação."
      ],
      "metadata": {
        "id": "z7I6Hx3QxZUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIDENCE_THRESHOLD = 0.7\n",
        "\n",
        "\n",
        "def predict_with_threshold(text, tokenizer, model, classifier, max_len, threshold):\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoded['input_ids'].to(model.device)\n",
        "    attention_mask = encoded['attention_mask'].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    cls_embedding = cls_embedding.cpu().numpy()\n",
        "    probas = classifier.predict_proba(cls_embedding)[0]\n",
        "\n",
        "    max_proba = np.max(probas)\n",
        "    if max_proba < threshold:\n",
        "        return \"Não sei\", max_proba\n",
        "\n",
        "    return (\"Verdadeiro\" if np.argmax(probas) == 1 else \"Falso\"), max_proba"
      ],
      "metadata": {
        "id": "SPGWROFbuKOE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testando com frases novas"
      ],
      "metadata": {
        "id": "pi8oBj96KM4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"EUA lançou míssil nuclear.\", #resultado esperado = mentira\n",
        "    \"Brasil declarou guerra contra algum país.\", #resultado esperado = mentira\n",
        "    \"Guitarrista de banda famosa atira em espectador durante show de rock.\", #resultado esperado = mentira\n",
        "    \"Partido diz que apoia a investigação com a ampla apuração dos eventuais crimes cometidos\", #resultado esperado = verdade\n",
        "    \"Eduardo Cunha disse em depoimento à Justiça Federal que não recebeu dinheiro da empresa JBS\", #resultado esperado = verdade\n",
        "    \"O ministro do STF liberou Andrea Neves da prisão domiciliar\" #resultado esperado = verdade\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    result, confidence = predict_with_threshold(\n",
        "        sentence, tokenizer, bert_model, classifier, MAX_LEN, CONFIDENCE_THRESHOLD\n",
        "    )\n",
        "    print(f\"Frase: {sentence}\")\n",
        "    print(f\"Resposta: {result}, Probabilidade: {confidence:.2f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY8TwFT5uMYs",
        "outputId": "9cc623f7-46c0-4d89-bd09-f0a726ef7a5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: EUA lançou míssil nuclear.\n",
            "Resposta: Falso, Probabilidade: 1.00\n",
            "\n",
            "Frase: Brasil declarou guerra contra algum país.\n",
            "Resposta: Falso, Probabilidade: 1.00\n",
            "\n",
            "Frase: Guitarrista de banda famosa atira em espectador durante show de rock.\n",
            "Resposta: Falso, Probabilidade: 1.00\n",
            "\n",
            "Frase: Partido diz que apoia a investigação com a ampla apuração dos eventuais crimes cometidos\n",
            "Resposta: Verdadeiro, Probabilidade: 1.00\n",
            "\n",
            "Frase: Eduardo Cunha disse em depoimento à Justiça Federal que não recebeu dinheiro da empresa JBS\n",
            "Resposta: Verdadeiro, Probabilidade: 1.00\n",
            "\n",
            "Frase: O ministro do STF liberou Andrea Neves da prisão domiciliar\n",
            "Resposta: Verdadeiro, Probabilidade: 0.90\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussão dos resultados e trabalhos futuros**\n",
        "\n",
        "Após o treinamento do modelo, efetuamos testes fornecendo frases relacionadas a notícias verdadeiras e notícias falsas que ele possuí conhecimento, e os resultados aparesentaram acuracidade alta.\n",
        "\n",
        "Como a base que utilizamos foi atualizada pela última vez quatro anos atrás, para uma continuação desse projeto seria interessante adicionar noticías referentes à esses anos faltantes, principalmente notícias mais recentes. Essa base deveria ser atualizada constantemente e o modelo retreinado, para que ele sempre possa responder com a maior acuracidade possível para o contexo atual. Como é mais difícil encontrar bases robustas em português, talvez fosse interessante nós mesmo fazermos esse processo de obtenção atráves de portais de notícia confiáveis e portais conhecidos pela divulgação de fake news."
      ],
      "metadata": {
        "id": "xjNEn42Vd2zW"
      }
    }
  ]
}